{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport datetime\nimport cv2 \nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential,load_model\nfrom tensorflow.keras.layers import Activation,Conv2D,Dense,MaxPool2D,Dropout,Flatten\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\nfrom tensorflow.keras.utils import plot_model\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure, imshow, axis\nfrom matplotlib.image import imread\nfrom IPython.display import Image, display\n\nfrom tensorflow.keras.applications import InceptionV3,Xception,EfficientNetB3\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras import Model\nfrom keras.layers import Flatten\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.regularizers import l2\n\nBase_directory = '/kaggle/input/flamedatasetfireclassification'\ntest_path = 'Test/Test'\nTraining_path = 'Training/Training'\ninput_shape = (254,254,3)\nbatch = 32\nlabels = ['Fire','No_Fire']\nFull_Training_path = '{0}/{1}'.format(Base_directory,Training_path)\nFull_Test_path = '{0}/{1}'.format(Base_directory,test_path)\n\ntrain_images = ImageDataGenerator(rotation_range=45,\n                                # width_shift_range=0.1,\n                                 #height_shift_range=0.1,\n                                 horizontal_flip=True,\n                                 vertical_flip=True,\n                                 rescale=1.0/255,\n                                 zoom_range=0.4,\n                                 shear_range=0.2,\n                                 fill_mode='nearest',\n                                 validation_split=0.2                                           \n#            preprocessing_function = contrast_stretching,\n#             preprocessing_function = HE,\n                                #preprocessing_function = AHE\n                             )\n\ntrain_generator = train_images.flow_from_directory(Full_Training_path, \n                                               target_size=(254,254),\n                                               color_mode='rgb',\n                                               class_mode='binary',\n                                               batch_size=batch,\n                                               shuffle=True,\n                                               subset='training')\n\nvalidation_generator = train_images.flow_from_directory(Full_Training_path, \n                                                    target_size=(254,254),\n                                                    color_mode='rgb',\n                                                    class_mode='binary',\n                                                    batch_size=batch,\n                                                    shuffle=True,\n                                                    subset='validation')\n\ntest_images = ImageDataGenerator(rescale=1.0/255)\n\ntest_generator = test_images.flow_from_directory(Full_Test_path, \n                                                target_size=(254,254), \n                                                color_mode='rgb', \n                                                class_mode='binary',\n                                                shuffle=False,\n                                                batch_size=batch)\n    \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Found 31501 images belonging to 2 classes.\nFound 7874 images belonging to 2 classes.\nFound 8617 images belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(32, (3, 3), input_shape=input_shape))\nmodel.add(Activation('relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 252, 252, 32)      896       \n_________________________________________________________________\nactivation (Activation)      (None, 252, 252, 32)      0         \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 126, 126, 32)      0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 124, 124, 32)      9248      \n_________________________________________________________________\nactivation_1 (Activation)    (None, 124, 124, 32)      0         \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 62, 62, 32)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 60, 60, 64)        18496     \n_________________________________________________________________\nactivation_2 (Activation)    (None, 60, 60, 64)        0         \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 30, 30, 64)        0         \n_________________________________________________________________\nflatten (Flatten)            (None, 57600)             0         \n_________________________________________________________________\ndense (Dense)                (None, 64)                3686464   \n_________________________________________________________________\nactivation_3 (Activation)    (None, 64)                0         \n_________________________________________________________________\ndropout (Dropout)            (None, 64)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 65        \n_________________________________________________________________\nactivation_4 (Activation)    (None, 1)                 0         \n=================================================================\nTotal params: 3,715,169\nTrainable params: 3,715,169\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"my_callbacks = [\n    EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1),\n    ModelCheckpoint(filepath='Xceptionnet_1104_{epoch:02d}_{val_accuracy:.04f}.h5',\n                    save_best_only=False)]\n\nhistory = model.fit(train_generator,\n            epochs=50,\n            validation_data = test_generator,            \n            callbacks=my_callbacks,\n            verbose=1    )","metadata":{"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Epoch 1/50\n985/985 [==============================] - 774s 782ms/step - loss: 0.4172 - accuracy: 0.8454 - val_loss: 1.0715 - val_accuracy: 0.5996\nEpoch 2/50\n985/985 [==============================] - 480s 487ms/step - loss: 0.2280 - accuracy: 0.9240 - val_loss: 1.3613 - val_accuracy: 0.6094\nEpoch 3/50\n985/985 [==============================] - 471s 478ms/step - loss: 0.2271 - accuracy: 0.9354 - val_loss: 1.1945 - val_accuracy: 0.6243\nEpoch 4/50\n985/985 [==============================] - 479s 486ms/step - loss: 0.1800 - accuracy: 0.9468 - val_loss: 0.9610 - val_accuracy: 0.6553\nEpoch 5/50\n985/985 [==============================] - 473s 480ms/step - loss: 0.3864 - accuracy: 0.9472 - val_loss: 1.9874 - val_accuracy: 0.6043\nEpoch 6/50\n985/985 [==============================] - 465s 472ms/step - loss: 0.2272 - accuracy: 0.9511 - val_loss: 3.8328 - val_accuracy: 0.6862\nEpoch 7/50\n985/985 [==============================] - 459s 466ms/step - loss: 0.3003 - accuracy: 0.9532 - val_loss: 14.9388 - val_accuracy: 0.6604\nEpoch 8/50\n985/985 [==============================] - 462s 469ms/step - loss: 0.2589 - accuracy: 0.9554 - val_loss: 6.7090 - val_accuracy: 0.7096\nEpoch 9/50\n985/985 [==============================] - 458s 465ms/step - loss: 0.1887 - accuracy: 0.9568 - val_loss: 1.1700 - val_accuracy: 0.7065\nEpoch 10/50\n985/985 [==============================] - 462s 469ms/step - loss: 0.1434 - accuracy: 0.9583 - val_loss: 2.8539 - val_accuracy: 0.7051\nEpoch 11/50\n985/985 [==============================] - 459s 466ms/step - loss: 0.3915 - accuracy: 0.9579 - val_loss: 9.6161 - val_accuracy: 0.6481\nEpoch 12/50\n985/985 [==============================] - 463s 470ms/step - loss: 0.6799 - accuracy: 0.9586 - val_loss: 11.1397 - val_accuracy: 0.7189\nEpoch 13/50\n985/985 [==============================] - 464s 471ms/step - loss: 0.2810 - accuracy: 0.9596 - val_loss: 15.0087 - val_accuracy: 0.6775\nEpoch 14/50\n985/985 [==============================] - 462s 469ms/step - loss: 0.1805 - accuracy: 0.9592 - val_loss: 1.5858 - val_accuracy: 0.7226\n\nEpoch 00014: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","output_type":"stream"}]}]}